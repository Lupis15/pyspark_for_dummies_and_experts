{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc26ebe0-9fe4-49ce-9970-c45aa5c280db",
   "metadata": {},
   "source": [
    "#### El cliente ha corroborado que podemos manejar RDD's pero el CORE a utilizar será dataframes, nos han solicitado crear algunas funciones para cargar archivos en formato csv y almacenarlos en formato parquet. ¿Por qué en parquet? Hecha un ojo a esto (vendrá en la evaluación final):\n",
    "#### https://www.databricks.com/glossary/what-is-parquet\n",
    "#### Cuando termines resuelve cada actividad planteada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba45903-1c28-444b-9af0-6442a36468a6",
   "metadata": {},
   "source": [
    "#### Actividad 1:\n",
    "##### TO DO: Crear sparkSession con appName \"ejercicio_4\", y master \"local[*]\", almacenar sesion de spark en una variable llamada \"spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a07f4ae-96d0-48c9-ad50-753c52b0ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaración de librerias\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71db4bec-ce2a-4b44-98d0-4fbdab40ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TU CODIGO VA EN ESTA CELDA:\n",
    "nombre_ejercicio = \"ejercicio_4\"\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ejercicio_4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a96f213c-5230-4a4d-9b01-d28e073d0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR CONTENIDO DE ESTA CELDA\n",
    "assert type(spark) == SparkSession\n",
    "assert spark.sparkContext.getConf().get(\"spark.master\") == \"local[*]\"\n",
    "assert spark.sparkContext.getConf().get(\"spark.app.name\") == \"ejercicio_4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c7529-e81d-4bc4-b1b4-abfe69e4b6a5",
   "metadata": {},
   "source": [
    "#### Actividad 2:\n",
    "##### TO DO ->    Crear un método para leer un archivo en formato csv, no inferir el esquema, los archivos si incluyen header, el separador es \",\"\n",
    "- ##### retorna ->  dataframe correspondiente a la lectura\n",
    "- ##### firma ->    def read_csv(path: str) -> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "636335bd-d238-410c-ab45-bcdb6c732fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TU CODIGO VA EN ESTA CELDA:\n",
    "def read_csv(path):\n",
    "    return spark.read\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .option(\"delimiter\",\",\")\\\n",
    "        .option(\"inferSchema\",\"false\")\\\n",
    "        .csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adb49e98-a62e-4324-af0b-626ec21c02ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NO MODIFICAR CONTENIDO DE ESTA CELDA\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "root_path = \"../resources/data/csv/\"\n",
    "\n",
    "movies_df = read_csv(root_path + \"movies.csv\")\n",
    "ratings_df = read_csv(root_path + \"ratings.csv\")\n",
    "tags_df = read_csv(root_path + \"tags.csv\")\n",
    "\n",
    "def schema_to_ddl(df):\n",
    "    return spark.sparkContext._jvm.org.apache.spark.sql.types.DataType.fromJson(df.schema.json()).toDDL()\n",
    "\n",
    "assert (type(movies_df) == DataFrame) & (type(ratings_df) == DataFrame) & (type(tags_df) == DataFrame)\n",
    "assert (movies_df.count() == 86537) & (ratings_df.count() == 33832162) & (tags_df.count() == 2328315)\n",
    "assert schema_to_ddl(movies_df) == 'movieId STRING,title STRING,genres STRING'\n",
    "assert schema_to_ddl(ratings_df) == 'userId STRING,movieId STRING,rating STRING,timestamp STRING'\n",
    "assert schema_to_ddl(tags_df) == 'userId STRING,movieId STRING,tag STRING,timestamp STRING'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cb1f3-2308-4ea6-94da-1d61f040941b",
   "metadata": {},
   "source": [
    "#### Actividad 3:\n",
    "##### TO DO ->    Crear un método para escribir un dataframe con las siguientes caracteristicas:\n",
    "- ##### formato: parquet\n",
    "- ##### modo de escritura: overwrite\n",
    "- ##### firma: def write_parquet(df: DataFrame, path: str):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08393772-6be3-4bc8-acea-1856800a3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TU CODIGO VA EN ESTA CELDA:\n",
    "def write_parquet(df: DataFrame, path: str):\n",
    "    pass\n",
    "    df.write.mode(\"overwrite\").parquet(\"../resources/data/tmp/parquet/04/\" + root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ee1cea1-a37e-4027-9520-abfca31b3bdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/Users/SoporteTI/pyspark_for_dummies_and_experts/resources/data/tmp/parquet/04/movies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m write_parquet(ratings_df, root_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m write_parquet(tags_df, root_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m _movies_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovies\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m _ratings_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(root_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m _tags_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(root_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\pyspark_for_dummies_and_experts\\venv\\lib\\site-packages\\pyspark\\sql\\readwriter.py:531\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    520\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    522\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[0;32m    523\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[0;32m    529\u001b[0m )\n\u001b[1;32m--> 531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\pyspark_for_dummies_and_experts\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\pyspark_for_dummies_and_experts\\venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/SoporteTI/pyspark_for_dummies_and_experts/resources/data/tmp/parquet/04/movies."
     ]
    }
   ],
   "source": [
    "# NO MODIFICAR CONTENIDO DE ESTA CELDA\n",
    "\n",
    "root_path = \"../resources/data/tmp/parquet/04/\"\n",
    "\n",
    "write_parquet(movies_df, root_path + \"movies\")\n",
    "write_parquet(ratings_df, root_path + \"ratings\")\n",
    "write_parquet(tags_df, root_path + \"tags\")\n",
    "\n",
    "_movies_df = spark.read.parquet(root_path + \"movies\")\n",
    "_ratings_df = spark.read.parquet(root_path + \"ratings\")\n",
    "_tags_df = spark.read.parquet(root_path + \"tags\")\n",
    "\n",
    "assert _movies_df.count() == 86537\n",
    "assert _ratings_df.count() == 33832162\n",
    "assert _tags_df.count() == 2328315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d814457-8f21-435f-94cd-ccbc37710670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc972b-fc55-468c-8928-afa9c8d95d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
