{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df4e982-562f-4ee6-a7e9-4fecda27c496",
   "metadata": {},
   "source": [
    "- #### Arquitectura\n",
    "- #### Flujo de ejecución\n",
    "- #### Transformaciónes vs Acciones\n",
    "- #### Optimizador Catalist\n",
    "- #### DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471db50d-a05e-44c9-8541-af21d996d01b",
   "metadata": {},
   "source": [
    "## Spark architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3810ce87-9687-4977-9e12-a5eaac240f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../resources/img/SPARK.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(url='../resources/img/SPARK.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da20d3-254a-4467-8fc0-a9d4850bfa05",
   "metadata": {},
   "source": [
    "#### Driver program\n",
    "The process running the main() function of the application and creating the SparkContext. **Starts the application and sends work to the executors.**\n",
    "\n",
    "#### Cluster manager\n",
    "An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN, Kubernetes)\n",
    "\n",
    "   - **Standalone** – a simple cluster manager included with Spark that makes it easy to set up a cluster.\n",
    "   - **Apache Mesos** – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)\n",
    "   - **Hadoop YARN** – the resource manager in Hadoop 2 and 3.\n",
    "   - **Kubernetes** – an open-source system for automating deployment, scaling, and management of containerized applications.\n",
    "\n",
    "#### Worker node\t\n",
    "Any node that can run application code in the cluster\n",
    "\n",
    "#### Executor\n",
    "A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors. **Launched in JVM containers with their own memory and CPU resources**\n",
    "\n",
    "#### Deploy mode\t\n",
    "Distinguishes where the driver process runs. \n",
    "\n",
    "- **Cluster mode** - the Spark driver is launched on a worker node\n",
    "- **Client mode** - the Spark driver is on the client machine\n",
    "- **Local mode** - the entire application runs on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490339b3-5622-4c8b-a3be-804726a1762e",
   "metadata": {},
   "source": [
    "## Spark app execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639bb527-b442-4b2f-beda-5ed1be6e446b",
   "metadata": {},
   "source": [
    "- An action triggers a **Job**\n",
    "  \n",
    "- A job is split into **stages**\n",
    "    - each stage is dependent on the stage before it\n",
    "    - a stage must fully complete before the next stage can start\n",
    "    - for performance (usually) minimize the number of stages\n",
    "\n",
    "- A stage has **tasks**\n",
    "    - task = smallest unit of work\n",
    "    - tasks are run by executors\n",
    "\n",
    "- **An RDD/DF/DS has partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4e4b79-5329-4734-b919-708b64e069f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"sesion_1\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62276910-79b2-48a8-8566-d93c9d126b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 1\n",
    "employees = sc.textFile(\"../resources/data/csv/employees.csv\")\n",
    "empTokens = employees.map(lambda line: line.split(\",\"))\n",
    "empDetails = empTokens.map(lambda tokens: (tokens[4], tokens[7]))\n",
    "## Stage 2\n",
    "empGroups = empDetails.groupByKey(2)\n",
    "avgSalaries = empGroups.mapValues(lambda salaries: sum([int(item) for item in salaries]) / len(salaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0864d994-8707-4475-b57a-f72681a4928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgSalaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352820d2-4e6f-4bd2-8819-3fd1380de697",
   "metadata": {},
   "source": [
    "#### **TRANSFORMATIONS ARE LAZY - SO ARE EXECUTED WHEN AN ACTION IS TRIGGERED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a307b490-6f59-4adc-ba18-f92ea12a0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgSalaries.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfac8f0-acbc-4434-8e6a-a0165c4bf763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../resources/img/Stages_tasks.PNG\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(url='../resources/img/Stages_tasks.PNG'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c791c-6393-473d-9520-dd7e30c42fb8",
   "metadata": {},
   "source": [
    "#### Shuffle\n",
    "\n",
    "- Exchange of data between executors\n",
    "- happens in between stages\n",
    "- must complete before next stage starts\n",
    "\n",
    "- Expensive because of\n",
    "    - transferring data\n",
    "    - serialization/deserialization\n",
    "    - loading new data from shufflefiles\n",
    "- Shuffles are performance bottlenecks because\n",
    "    - exchanging data takes time\n",
    "    - they need to be fully completed before next computations start\n",
    "- Shuffles limit parallelization\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d7fd9-5093-413f-8d0a-c2e2e79f5daf",
   "metadata": {},
   "source": [
    "#### Concepts Relationship\n",
    "App decomposition\n",
    "- 1 job = 1 or more stages\n",
    "- 1 stage = 1 or more tasks\n",
    "\n",
    "Tasks & executors\n",
    "- 1 task is run by 1 executor\n",
    "- each executor can run 0 or more tasks\n",
    "\n",
    "Partitions and tasks\n",
    "- processing 1 partition = one task\n",
    "\n",
    "Partitions & executors\n",
    "- 1 partition stays on 1 executor\n",
    "- each executor can load 0 or more partitions in memory or disk\n",
    "\n",
    "Executors & nodes\n",
    "- 1 executor = 1 JVM on 1 physical node\n",
    "- each physical node can have 0 or more executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead6f94-a013-4f2c-b13f-9fd28b92b6fc",
   "metadata": {},
   "source": [
    "## Transformations vs Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e816447-19b5-49eb-8596-81f48ca6c911",
   "metadata": {},
   "source": [
    "- Transformations describe how new DFs are obtained\n",
    "- Actions start executing Spark code\n",
    "- Transformations return RDDs/DFs\n",
    "- Actions return something else e.g. Unit, number, etc.\n",
    "\n",
    "#### Narrow Transformations\n",
    "\n",
    "- given a parent partition, a single child partition depends on it\n",
    "- fast to compute\n",
    "- examples: union, map, flatMap, select, filter\n",
    "\n",
    "#### Wide transformations\n",
    "\n",
    "- given a parent partition, more than one child partitions depend on it\n",
    "- involve a shuffle = data transfer between Spark executors\n",
    "- are costly to compute\n",
    "- examples: joining, grouping, sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58568431-33d1-4876-bba3-a42832920bac",
   "metadata": {},
   "source": [
    "## Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3055577a-9381-406b-a3bb-a31d74735c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../resources/img/Catalist_Optimizer.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(url='../resources/img/Catalist_Optimizer.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5d348-df9e-4cc2-87e4-dc609c4ec89f",
   "metadata": {},
   "source": [
    "1. Catalyst Optimizer and Tungsten Execution Engine was introduced in Spark 1.x\n",
    "2. Cost-Based Optimizer was introduced in Spark 2.x\n",
    "3. Adaptive Query Execution now got introduced in Spark 3.x\n",
    "\n",
    "    - To disable the Adaptive Query Execution -> spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "\n",
    "Only works with DF and DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77f8997a-da68-4f75-8844-22c9ffcd5a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[sum(id#126L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=699]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#126L)])\n",
      "         +- Project [id#126L]\n",
      "            +- SortMergeJoin [id#126L], [id#120L], Inner\n",
      "               :- Sort [id#126L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(id#126L, 200), ENSURE_REQUIREMENTS, [plan_id=691]\n",
      "               :     +- Project [(id#118L * 3) AS id#126L]\n",
      "               :        +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=681]\n",
      "               :           +- Range (1, 100000000, step=1, splits=8)\n",
      "               +- Sort [id#120L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#120L, 200), ENSURE_REQUIREMENTS, [plan_id=692]\n",
      "                     +- Exchange RoundRobinPartitioning(9), REPARTITION_BY_NUM, [plan_id=684]\n",
      "                        +- Range (1, 100000000, step=2, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds1 = spark.range(1, 100000000)\n",
    "ds2 = spark.range(1, 100000000, 2)\n",
    "ds3 = ds1.repartition(7)\n",
    "ds4 = ds2.repartition(9)\n",
    "ds5 = ds3.selectExpr(\"id * 3 as id\")\n",
    "joined = ds5.join(ds4, \"id\")\n",
    "sum = joined.selectExpr(\"sum(id)\")\n",
    "sum.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e1e8d-0af3-422f-8e11-83980de000aa",
   "metadata": {},
   "source": [
    "## DAG\n",
    "\n",
    "DAG = Directed Acyclic Graph = visual representation of Spark jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de3c540e-2ae2-401b-9ff4-31e9221a95a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://23LAP5CD0176GH7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sesion_1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2c79a7dfe80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd1ecfda-d392-4e89-a663-0059b42dc47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum.count() # Call an action to trigger transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a8e421-3d26-46e7-ab0d-32c40e8c9c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../resources/img/DAG.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(url='../resources/img/DAG.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
